---
title: "Data Analysis 3 - Assignment 1"
author: "Ali Sial"
date: "1/26/2022"
output: pdf_document
header-includes: 
  - \usepackage{float}
  - \usepackage{longtable}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

getwd()
# CLEAR MEMORY
rm(list=ls())

# Import libraries
library(tidyverse)
library(modelsummary)
library(fixest)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(cowplot)
library(gridExtra)
library(ggpubr)
library(ggplot2)
library(kableExtra)
library(data.table)
```

## Introduction

The aim of this document is to build models and **predict earnings per hour** using linear regression. The data used for this analysis is retrieved from **_[cps-earnings dataset](https://osf.io/g8p9j/)_** (please click for the link) and the profession selected is **Secretaries and Administrative Assistants** (Occupation Code: 5700). The data has a total of 3511 observations for this occupation, majority being females. 

## Data Cleaning and Mungging
The first step after loading the data was to filter observations for full-time employees (at least 40hrs per week) and also removed variables with missing values. After this our target variable was added to the data that is "Earnings per Hour". Upon completing the basic cleaning, I investigated each predictor to check if further feature engineering is required. A squared term was added for "Age" to incorporate all observations when running linear regression. I also dropped observation that had education level higher than Masters and regrouped the remaining in a new variable. Dummy variables were created for gender, race and own child. To avoid complexity the States were regrouped into four regions. The final observations that will be used for analysis are **2576**. 

```{r, include=FALSE}
#################
#  DATA IMPORT  #
#################

data_raw <- read.csv('https://osf.io/4ay9x/download', stringsAsFactors = TRUE)
```


```{r, include=FALSE}
################
# Data Munging #
################

# Selecting Occupation: "5700" i.e Secretaries and administrative assistants. 
df <- data_raw %>% 
  filter(occ2012==5700)


# Filtering data for only full time employees i.e. >=40 Hrs per week
df <- df %>%
  filter(uhours>=40) 

# Creating the target variable: Earnings per Hour as "eph"
df$eph <- (df$earnwke/df$uhours)


# Quick look at all variables
datasummary_skim(df)


# Checking for missing values count in the variables 
to_filter <- sapply(df, function(x) sum(is.na(x)))
to_filter[to_filter > 0]             

# Dropping missing: in this case ethnic variable has 89% missing values
df$ethnic <- NULL     


```


```{r, include=FALSE}
###########################
# Investigating Variables #
###########################


########
## Earnings per hour (target variable)
ggplot(df , aes(x = eph)) +
  geom_histogram( fill='navyblue', color = 'white' ) +
  theme_bw()

## it appears that the distribution has a right long tail, 
## therefore taking the log for this variable
ggplot(df , aes(x = log(eph))) +
  geom_histogram( fill='navyblue', color = 'white' ) +
  theme_bw()
## log for earnings per hour not included as it doesnt adjust for normal distribution


########
## Age
ggplot( df , aes(x = age, y = eph)) +
  geom_point(color='red',size=2,alpha=0.6) +
  geom_smooth(method="loess" , formula = y ~ x )+
  theme_bw()


## Adding age squared column
df$age2 <- df$age^2

########
## Educational Degree (grade92)
ggplot( df , aes(x = grade92, y = eph)) +
  geom_point(color='red',size=2,alpha=0.6) +
  geom_smooth(method="loess" , formula = y ~ x )+
  theme_bw()

## Dropping values unrealistic values in educational level for secretaries,
## in this case PhD and Doctorate will be dropped since its unlikely for 
## people working within this profession after such educational level

df <- df %>% 
  filter(grade92 <= 44)

## Regrouping the levels in grade92 variable based on the graph above for ep and grade92
## It made sense to combine the eduction level below 39 since whether the employer has
## completed 9th grade or 10th grade, both count as not having a diploma.
## 


df <- df %>% mutate(edu=case_when(
  grade92<=38 ~ "No Diploma",                 # for education levels of less than 12th grade
  grade92==39 ~ "Diploma",                    # High school, diploma, GED
  grade92==40 ~ "College without Degree",    # college education without degree
  grade92<=42 ~ "Associate Degree",           # Associate Degree
  grade92==43 ~ "Bachelors Degree",
  grade92==44 ~ "Masters Degree"
))

########
## Race
## Adding dummy variable for race, white and not white
df$race_dummy <- ifelse(df$race == 1, 'white','other')

########
## Sex 
## Redefining the sex variable as Male and Female
df$gender <- ifelse(df$sex == 1, 'male', 'female')

#######
## Marital Status
## Regrouping the marital status "Married, "Single Parent", "Never Married"

df <- df %>% mutate(marital_status=case_when(
  marital<=2 ~ "Married",
  marital<=6 ~ "Single Parent",
  marital==7 ~ "Never Married"
))

#######
## Own Child
## Creating binary for own child 
df <- df %>% mutate(ownchild=case_when(
  ownchild==0 ~ 0,
  TRUE ~ 1))

#######
## Class
datasummary( eph*factor(class) ~ N + Percent() + Mean, data = df ) 
# will include as factor because this kind occupation has a difference in earnings 
# based on class - so it will be added as is

#######
## Union Membership 
datasummary( eph*factor(unionmme) ~ N + Percent() + Mean, data = df ) 
## will be included in the model


#######
## Stfips 

datasummary( eph*factor(stfips) ~ N + Percent() + Mean, data = df ) 

## Stfips and state variable are kind of similar with alot of different values
## Therefor, i'll be grouping these states in 4 regions to avoid complexity

df <- df %>% mutate(region=case_when(
  stfips %in% c("WA", "OR", "MT", "ID", "WY", "NV", "UT", "CO", "AZ", "NM", "HI", "AK", "CA") ~ "West",
  stfips %in% c("ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH") ~ "Mid-West",
  stfips %in% c("OK", "TX", "AR", "LA", "KY", "TN", "MS", "AL", "WV", "VA", "NC", "SC", "GA", "FL", "DC","MD","DE") ~ "South",
  stfips %in% c("PA", "NY", "VT", "NH", "ME","MA","RI","CT","NJ") ~ "North East"
))


# Removing all unwanted variable
df_clean <- df %>% select(-c(hhid,intmonth,stfips,weight,earnwke,uhours,grade92,
                             race,sex,marital,chldpres,prcitshp,state,ind02,unioncov,
                             lfsr94,occ2012,state))

## Variables dropped that were not explained or explored above are dropped
## because either there wasn't much variation in the observations
## or a similar variable was already included for further modeling (e.g: prcitshp
## majority of the observations were from the same category thus adding not much value 
## to the data available). 

```


## Selecting Interactions and Prediction Models

Understanding interactions within the variables was a challenging task, but in the end out of the 11 predictors used for modeling, gender, race and union membership had significant interaction terms. Total 11 interaction terms are used to build the models, majority for gender. The most complex interaction that uses 3 predictors is between gender, race and education level. Four models were built using predictors and interactions. The first model is the simplest with only one variable which is education level. In the second model i also included age, age square and gender. The third model includes all the variables and an interaction term for gender and education level. The last model is the most complex with all predictors and the interaction terms. Next step was to run regression and cross validate the performance of models. 


```{r, include=FALSE}
##########################
## Setting Interactions ##
##########################

# Investigating interactions within Variables #
## checking to see if further variables or reclassification of variables is required

### 1
## Earnings, Education and Gender
datasummary( eph*factor(edu)*gender ~ N + Percent() + Mean, data = df_clean )
# the summary suggest that their is a difference in earnings based on education and race

gg1 <- ggplot(df_clean, aes(x = factor(edu), y = eph,
                            fill = factor(gender), color=factor(gender))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Education Level",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw() +
  ggtitle("Earnings, Education and Gender") +
  theme(axis.text.x = element_text(angle=45, vjust=.5))

gg1


### 2
## Earnings, Race and Gender
datasummary( eph*factor(race_dummy)*gender ~ N + Percent() + Mean, data = df_clean )
# it appears that majority of the employees are white and earnings differ based on summary

gg2 <- ggplot(df_clean, aes(x = factor(race_dummy), y = eph,
                            fill = factor(gender), color=factor(gender))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Race",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw() +
  ggtitle("Earnings, Race and Gender")

gg2


### 3
## Earnings, Education Level, Race and Gender
datasummary( eph*edu*race_dummy*gender ~ N + Percent() + Mean, data = df_clean )
# There appears to be a difference in earngs based on all the 3 variables
# Graph below only for Education and Race comparison

gg3 <- ggplot(df_clean, aes(x = factor(edu), y = eph,
                            fill = factor(race_dummy), color=factor(race_dummy))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Education Level",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw() +
  ggtitle("Earnings, Education Level, Race and Gender")
  theme(axis.text.x = element_text(angle=45, vjust=.5))


gg3

### 4
## Earnings, Marital Status and Gender
datasummary( eph*marital_status*gender ~ N + Percent() + Mean, data = df_clean )
# It appears that earnings is different for marital status and gender

gg4 <- ggplot(df_clean, aes(x = marital_status, y = eph,
                            fill = factor(gender), color=factor(gender))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Marital Status",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw() +
  ggtitle("Earnings, Marital Status and Gender")

gg4


### 5
## Earnings, Union Member and Gender
datasummary( eph*unionmme*gender ~ N + Percent() + Mean, data = df_clean )
# It appears that earnings is different for union member and gender

gg5 <- ggplot(df_clean, aes(x = unionmme, y = eph,
                            fill = factor(gender), color=factor(gender))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Union Member",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw() + 
  ggtitle("Earnings, Union Member and Gender")

gg5


### 6
## Earnings, Own Child and Gender
datasummary( eph*factor(ownchild)*gender ~ N + Percent() + Mean, data = df_clean )
# It appears that earnings is different especially for males for if they have a child

gg6 <- ggplot(df_clean, aes(x = factor(ownchild), y = eph,
                            fill = factor(gender), color=factor(gender))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Own Child",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw() +
  ggtitle("Earnings, Own Child and Gender")

gg6


### 7
## Earnings, Class and Gender
datasummary( eph*factor(class)*gender ~ N + Percent() + Mean, data = df_clean )
# It appears that earnings is different based on class and gender

gg7 <- ggplot(df_clean, aes(x = factor(class), y = eph,
                            fill = factor(gender), color=factor(gender))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Class",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw() +
  theme(axis.text.x = element_text(angle=45, vjust=.5)) +
  ggtitle("Earnings, Class and Gender")

gg7


### 8
## Earnings, Region and Gender
datasummary( eph*region*gender ~ N + Percent() + Mean, data = df_clean )
# It appears that earnings is different based on gender and region

gg8 <- ggplot(df_clean, aes(x = factor(region), y = eph,
                            fill = factor(gender), color=factor(gender))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Region",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw() +
  theme(axis.text.x = element_text(angle=45, vjust=.5)) +
  ggtitle("Earnings, Region and Gender")

gg8




### 9
## Earnings, Race and Marital Status
datasummary( eph*race_dummy*marital_status ~ N + Percent() + Mean, data = df_clean )
# It appears that earnings is different based on race and marital status

gg9 <- ggplot(df_clean, aes(x = factor(marital_status), y = eph,
                            fill = factor(race_dummy), color=factor(race_dummy))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Race",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw() +
  ggtitle("Earnings, Race and Marital Status")

gg9


### 10
## Earnings, Union Member and Class
datasummary( eph*unionmme*class ~ N + Percent() + Mean, data = df_clean )
# It appears that earnings is different based on union membership and class

gg10 <- ggplot(df_clean, aes(x = factor(class), y = eph,
                             fill = factor(unionmme), color=factor(unionmme))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Class",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw()+
  theme(axis.text.x = element_text(angle=45, vjust=.5)) +
  ggtitle("Earnings, Union Member and Class")

gg10



### 11
## Earnings, Union Member and Region
datasummary( eph*unionmme*region ~ N + Percent() + Mean, data = df_clean )
# It appears that earnings is different based on union membership and region, especially
# in the case of south region


gg11 <- ggplot(df_clean, aes(x = factor(region), y = eph,
                             fill = factor(unionmme), color=factor(unionmme))) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  scale_color_manual(name="",
                     values=c('red','blue')) +
  scale_fill_manual(name="",
                    values=c('red','blue')) +
  labs(x = "Region",y = "Earnings per Hour (USD)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 50), breaks = seq(0,50, 10))+
  theme_bw()+
  theme(axis.text.x = element_text(angle=45, vjust=.5)) + 
  ggtitle("Earnings, Union Member and Region")

gg11

```


```{r, include=FALSE}
###################################
## Setting up Regressions models ##
###################################


# Model 1:
# As per the my own understanding the earnings tend to be higher the more educated the employee is, 
# Therefor, the first model is a basic model with only education as the prediction variable.
model1 <- as.formula(eph ~ edu)



# Model 2: 
# Usually the earnings are higher for older employees
# Thus, in this model apart from education, I have included  age, and square term of age.
# Gender variable is also included to incorporate the earnings difference due to gender
model2 <- as.formula(eph ~ edu + age + age2 + gender)



# Model 3:
# In this model i have included all variable that can impact the earnings of an individual along with
# few interaction terms for gender 
model3 <- as.formula(eph ~ edu + age + age2 + gender + gender*edu + gender*class + gender*marital_status
                     + race_dummy + ownchild + unionmme + marital_status  + class + region)


# Model 4:
# This is the most complex model that includes all variable plus possible interaction terms
# for gender, race and union membership 
model4 <- as.formula(eph ~ edu + age + age2 + gender + race_dummy + ownchild + unionmme +
                       marital_status  + class + region + gender*edu + gender*class + 
                       gender*marital_status + gender*race_dummy + gender*unionmme +
                       gender*ownchild + race_dummy*marital_status + unionmme*class +
                       unionmme*region + edu*race_dummy*gender)


### Running the regressions
reg1 <- feols(model1, data = df_clean , vcov="hetero")
reg2 <- feols(model2, data = df_clean , vcov="hetero" )
reg3 <- feols(model3, data = df_clean , vcov="hetero" )
reg4 <- feols(model4, data = df_clean , vcov="hetero" )

summary(reg2)

# evaluation of the models: using all the sample
fitstat_register("k", function(x){length( x$coefficients ) - 1}, "No. Variables")           
summary<- kable(etable( reg1 , reg2 , reg3 , reg4 , fitstat = c('aic','bic','rmse','r2','n','k'),
                   keepFactors = TRUE), booktabs = TRUE,  position = "H",
       caption = '') %>% 
 kable_styling(latex_options = c("hold_position","scale_down"))

```



```{r, include=FALSE}
######################
# Cross-Validation  ##
######################


# Simple k-fold cross validation setup:
# 1) Used method for estimating the model: "lm" - linear model (y_hat = b0+b1*x1+b2*x2 + ...)
# 2) set number of folds to use (must be less than the no. observations)
k <- 4

# We use the 'train' function which allows many type of model training -> use cross-validation
set.seed(300)
cv1 <- train(model1, df_clean, method = "lm", trControl = trainControl(method = "cv", number = k))

# Check the output:
cv1
summary(cv1)
cv1$results
cv1$resample

set.seed(300)
cv2 <- train(model2, df_clean, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(300)
cv3 <- train(model3, df_clean, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")
set.seed(300)
cv4 <- train(model4, df_clean, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")

# Calculate RMSE for each fold and the average RMSE as well
cv <- c("cv1", "cv2", "cv3", "cv4")
rmse_cv <- c()

for(i in 1:length(cv)){
  rmse_cv[i] <- sqrt((get(cv[i])$resample[[1]][1]^2 +
                        get(cv[i])$resample[[1]][2]^2 +
                        get(cv[i])$resample[[1]][3]^2 +
                        get(cv[i])$resample[[1]][4]^2)/4)
}


# summarize results
cv_mat <- data.frame(rbind(cv1$resample[4], "Average"),
                     rbind(cv1$resample[1], rmse_cv[1]),
                     rbind(cv2$resample[1], rmse_cv[2]),
                     rbind(cv3$resample[1], rmse_cv[3]),
                     rbind(cv4$resample[1], rmse_cv[4])
)

colnames(cv_mat)<-c("Resample","Model 1", "Model 2", "Model 3", "Model 4")

cv_mat

# Show model complexity and out-of-sample RMSE performance
m_comp <- c()
models <- c("reg1", "reg2", "reg3", "reg4")
for( i in 1 : length(cv) ){
  m_comp[ i ] <- length( get( models[i] )$coefficient  - 1 ) 
}

m_comp <- tibble( model = models , 
                  complexity = m_comp,
                  RMSE = rmse_cv )

m_comp


gg12 <- ggplot( m_comp , aes( x = complexity , y = RMSE ) ) +
  geom_point(color='red',size=2) +
  geom_line(color='blue',size=0.5)+
  labs(x='Number of explanatory variables',y='Averaged RMSE on test samples',
       title='Prediction performance and Model compexity') +
  theme_bw() +
  ggtitle("Cross-Validation RMSE Comparison")

gg12

# plotting results
gg13<- ggplot(df_clean, aes(x=predict(reg2, df_clean), y=eph)) + 
  geom_point(color='red', alpha = 0.3) +
  geom_abline(intercept = 0, slope = 1, size = 0.5, color='blue') +
  scale_x_continuous(limits = c(0,30)) + 
  scale_y_continuous(limits = c(0,60)) +
  theme_bw() +
  ggtitle("Prefrred Prediction Model")

gg13
```



## Interpreting the Results

The performance of these prediction models was first compared using Root Mean Square Error (RMSE). After running the regressions model 4 had the lowest RMSE (the most complex model) i.e. 7.8509. Next, I cross validated RMSE using the K-Fold method by creating 4 training and test samples. The cross validated RMSE was lowest for model 3, but the difference between model 2 and model 3 was very minute. Based on this comparison, even though model 3 had the lowest RMSE, model 2 is less complex, thus I would prefer that over model 3. This can be further validated by using Bayesian Information Criterion (BIC), which adds a penalty for model complexity. When comparing the models using BIC, the model 2 had lowest BIC value. Therefore, we can conclude that model 2 would the most preferred model for predicting earnings per hour for this specific occupation. 






## Appendix

**Interactions with Gender**


```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 4, fig.align="center"}
g1<- ggarrange(gg1, gg4,
           hjust = -0.6,
            ncol = 2, nrow = 1)
g1
```


```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 4, fig.align="center"}
g2<- ggarrange(gg5, gg6,
           hjust = -0.6,
            ncol = 2, nrow = 1)
g2
```

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 4, fig.align="center"}

ggarrange(gg7, gg8,
           hjust = -0.6,
            ncol = 2, nrow = 1)

gg3

```

\newpage
**Interactions with Race**

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 4, fig.align="center"}
ggarrange(gg2, gg9,
           hjust = -0.6,
            ncol = 2, nrow = 1)
```


**Interactions with Union Membership**
```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 4, fig.align="center"}

ggarrange(gg10, gg11,
           hjust = -0.6,
            ncol = 2, nrow = 1)

```


\newpage

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 4, fig.align="center"}

reg_results <- etable( reg1 , reg2 , reg3 , reg4 , fitstat = c('aic','bic','rmse','r2','n','k'), keepFactors = TRUE )

reg_stats <- setDF(reg_results)

models <- c("Model 1", "Model 2", "Model 3", "Model 4")
rmse <- c(reg_stats$reg1[59], reg_stats$reg2[59], reg_stats$reg3[59] ,reg_stats$reg4[59])
bic <- c(reg_stats$reg1[58], reg_stats$reg2[58], reg_stats$reg3[58] ,reg_stats$reg4[58])
vars <- c(reg_stats$reg1[62], reg_stats$reg2[62], reg_stats$reg3[62] ,reg_stats$reg4[62])

reg_results_table <- data.frame(models, bic, rmse, vars)

colnames(reg_results_table)<- c("Model", "BIC", "RMSE","No. of coeff")

reg_results_table <- reg_results_table %>% mutate_if(is.numeric, format) %>% 
  kable( caption = "Model evaluation based on full sample RMSE and BIC") %>%
  kable_styling(latex_options = c("hold_position",full_width = T))

reg_results_table
```



```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 4, fig.align="center"}

cv_mat %>% kbl(caption = "Cross Validation Matrix", booktabs = T) %>%
  kable_styling(latex_options = c("hold_position",full_width = T))

m_comp %>% kbl(caption = "Model Complexity", booktabs = T) %>%
  kable_styling(latex_options = c("hold_position",full_width = T))

gg12
```


```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 4, fig.align="center"}
gg13
```

\newpage
**Regression Table**
```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 4, fig.align="center"}
summary
```
